# Setup Env
You can configure the generate container to connect to a bucket in your own Google Cloud account.

```bash
# Make a copy of the sample enviroment file.
cp .env.sample .env
```
Our production data is hosted in project 'mlops-399714' in us-east1 in a bucket named ac215-rsp-radar
that has public read access. However to deploy and test in your own Google Cloud account, update
the following parameters in the .env file: GOOGLE_CLOUD_PROJECT, GOOGLE_DEFAULT_REGION, GOOGLE_BUCKET_NAME.
You will also need to copy in your IAM user credentials into secrets/data-service-account.json.

# Docker Build & Run
```bash
# Build your docker image and give your image the name `generate`
docker build -t generate .
```

```bash
# You should be able to run your docker image by using:
docker run --rm -ti --mount type=bind,source="$(pwd)",target=/app generate
```


# Using the generate CLI to generate and inspect training examples
The cli has 6 subcommands:

### Setup
```bash
python cli.py setup
```
Checks that the bucket is created and has the necessary subdirectory, otherwise it creates it.

### Generate
```bash
python cli.py generate -n 10
```
This command calls the VertexAI API to generate n number of emails to be used as training data for the fine-tuned model and upload it to our Google Bucket. It used a few-shot prompt engineering technique to generate training examples of the correct format. Each generated training example looks like the following example:
```json
{
  "record": {
    "sender": "Sally Jones",
    "received_time": "2023-09-25 14:15",
    "subject": "Join us for a Holiday Party!",
    "body": "Hello everyone,

I am excited to invite you to our annual holiday party, which will be held on December 15th at 7:00 PM at the Holiday Inn. This year's party will be bigger and better than ever, with live music, dancing, and a buffet dinner. There will also be a silent auction with some amazing prizes up for grabs.

We hope to see you all there!

Best regards,
Sally Jones"
  },
  "label": [
    {
      "date": "December 15th 2023",
      "time": "7:00 PM",
      "subject": "Annual Holiday Party"
    }
  ]
}
```
We are generating both positive and negative examples (emails with events and without events, respectively), at a 1:2 ratio.
After validating the right JSON format expected, each training example is stored into a cloud storage bucket under [GOOGLE_BUCKET_NAME]/[LABELED_DIR] as its own file [randomly-generate-uuid].json. 

The -n argument command determines how many traing examples are generated by the command (which defaults to n=1).

### List
```bash
python cli.py list 
```
Lists all of the uuids of the traing examples in the storage bucket

### Show
```bash
python cli.py show [-i uuid] 
```
Shows one training example from the examples in the storage bucket based on the uuid passed into the -i
argument.  Defaults to showing a random example.

### Prepare
```bash
python cli.py prepare
```
Splits the data into train (75%), test (15%), and evaluation (15%) folders and converts them into the right format (jsonl file) to be passed onto our model training.

### TF Data
```bash
python cli.py tf_data 
```
While our training job on Vertex AI does not require (or allow) the training examples to be in the form of TFData, we have implemented a function in our generate/cli.py to convert these traing examples to TFData anyway. Even though not immediately required for our training jobs, it serves as a proactive measure to ensure flexibility in our machine learning pipeline. We wanted to make sure we understand how this library works, should we need it in the future. The command will upload the transformed data to the GCS bucket under the folder TF_DATA/.
